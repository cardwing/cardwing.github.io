
<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<head>
  <meta name=viewport content="width=device-width, initial-scale=1">
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <style type="text/css">
    /* Color scheme stolen from Sergey Karayev */

    a {
      color: #1772d0;
      text-decoration: none;
    }

    a:focus,
    a:hover {
      color: #f09228;
      text-decoration: none;
    }

    body,
    td,
    th,
    tr,
    p,
    a {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 14px
    }

    strong {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 14px;
    }

    heading {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 22px;
    }

    papertitle {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 14px;
      font-weight: 700
    }

    name {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 32px;
    }

    .one {
      width: 160px;
      height: 160px;
      position: relative;
    }

    .two {
      width: 160px;
      height: 160px;
      position: absolute;
      transition: opacity .2s ease-in-out;
      -moz-transition: opacity .2s ease-in-out;
      -webkit-transition: opacity .2s ease-in-out;
    }

    .fade {
      transition: opacity .2s ease-in-out;
      -moz-transition: opacity .2s ease-in-out;
      -webkit-transition: opacity .2s ease-in-out;
    }

    span.highlight {
      background-color: #ffffd0;
    }
  </style>
  <style type="text/css">
    #sectiontohide {
      padding: 20px;
      background: #f0f0f0;
      width: 400px;
    }
  </style>
  <script type="text/javascript">
    function toggle_div_fun(id) {

      var divelement = document.getElementById(id);

      if (divelement.style.display == 'none')
        divelement.style.display = 'block';
      else
        divelement.style.display = 'none';
    }
  </script>
  <title>HOU Yuenan (侯跃南)</title>
  <meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
  <link href='http://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
</head>

<body>
  <table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
    <tr>
      <td>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="67%" valign="middle">
              <p align="center">
                <name>HOU Yuenan (侯跃南)</name>
              </p>
              <p>
                Yuenan Hou is a Researcher of Shanghai AI Laboratory, led by Dr. <a href="https://jimmysuen.github.io/">Xiao Sun</a> and Prof. <a href="https://scholar.google.com/citations?hl=en&user=gFtI-8QAAAAJ">Yu Qiao</a>. He obtained the Ph.D. degree from the Chinese University of Hong Kong (<a href="http://mmlab.ie.cuhk.edu.hk/index.html">Multimedia Lab</a>) in August 2021, under the supervision of Prof.
                <a href="https://scholar.google.com/citations?user=559LF80AAAAJ&hl=en">Chen Change Loy</a> and Prof.
                <a href="https://scholar.google.com/citations?user=qpBtpGsAAAAJ&hl=en">Xiaoou Tang</a>. He received his B.E degree from Nanjing University in July 2017, supervised by Prof. <a href="https://scholar.google.com.hk/citations?user=5kXEo74AAAAJ&hl=en">Chunlin Chen</a> and Prof. <a href="https://www.researchgate.net/profile/Xianzhong_Zhou">Xianzhong Zhou</a>.
                His research interest includes <b>Autonomous Driving</b>, <b>Embodied AI</b> and <b>Efficient Learning</b>.
              </p>
              <p>
                If you are interested in research intern/engineer or <b>joint-training PhD programs</b> or <b>academic/industrial collaboration</b> in Shanghai AI Lab, please send the e-mail to houyuenan[at]pjlab.org.cn.
              </p>
              <p align=center>
                <a href="mailto:Yuenan_Hou@163.com"> Email </a> /
                <a href="https://scholar.google.com.hk/citations?user=o9mX9sUAAAAJ&hl=en"> Google Scholar </a> /
                <a href="https://github.com/cardwing"> Github </a>
              </p>
            </td>
            <td width="15%">
              <right><img src="Me_2024.png" width="200" height="144"></right>
            </td>
          </tr>
        </table>

        
        
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td>
              <heading>News</heading>
            </td>
          </tr>
        </table>
        <ul>
          <li>
            <p>One paper is accepted by ISPRS (IF=12.7). (2024-09)<font color="ff0000"><strong><em>NEW!</em></strong></font></p>
          </li>
        </ul>
        <ul>
          <li>
            <p>One paper is accepted by TPAMI (IF=20.8). (2024-08)</p>
          </li>
        </ul>
        <ul>
          <li>
            <p>One paper is accepted by ECCV. (2024-07)</p>
          </li>
        </ul>
        <ul>
          <li>
            <p>Two papers are accepted by CVPR. (2024-02)</p>
          </li>
        </ul>
        <ul>
          <li>
            <p>Two papers are accepted by AAAI, including one Oral. (2024-01)</p>
          </li>
        </ul>
        <!--
        <ul>
          <li>
            <p>Two papers are accepted by NeurIPS. (2023-09-22)</p>
          </li>
        </ul>        
        <ul>
          <li>
            <p>One paper is accepted by PR. (2023-08-10)</p>
          </li>
        </ul>
        <ul>
          <li>
            <p>Four papers are accepted by ICCV 2023. (2023-07-14)</p>
          </li>
        </ul>
        <ul>
          <li>
            <p>Serve as an Area Chair of PRCV 2023. (2023-07-03)</p>
          </li>
        </ul>
        <ul>
          <li>
            <p>One paper is accepted by MedIA. (2023-06-12)</p>
          </li>
        </ul>
        <ul>
          <li>
            <p>One paper is accepted by TWC. (2023-04-21)</p>
          </li>
        </ul>
        <ul>
          <li>
            <p>Three papers are accepted by CVPR 2023, including one highlight. (2023-02-28)</p>
          </li>
        </ul>
        <ul>
          <li>
            <p>Our team ranks 1st on <strong>six</strong> challenges of Waymo, SemanticKITTI and nuScenes. (2022-10-31)</p>
          </li>
        </ul> 
        <ul>
          <li>
            <p>Congrats to <a href="https://hathawayxxh.github.io/">Dr. Xing</a> with our paper won MICCAI Society Young Scientist Award 2022. (2022-09-22)</p>
          </li>
        </ul>
        <ul>
          <li>
            <p>Two papers are accepted by ECCV 2022. (2022-07-04)</p>
          </li>
        </ul>
        <ul>
          <li>
            <p>One paper is accepted by MICCAI 2022. (2022-06-03)</p>
          </li>
        </ul> 
        <ul>
          <li>
            <p>Two papers are accepted by CVPR 2022. (2022-03-02)</p>
          </li>
        </ul>     
        <ul>
          <li>
            <p>One paper is accepted by MICCAI 2021. (2021-06-12)</p>
          </li>
        </ul>  
        <ul>
          <li>
            <p>One paper is accepted by CVPR 2020. (2020-02-24)</p>
          </li>
        </ul>
        <ul>
          <li>
            <p>One paper is accepted by ICCV 2019. (2019-07-23)</p>
          </li>
        </ul>
        <ul>
          <li>
            <p>One paper is accepted by AAAI 2019 as <strong>Oral</strong>. (2018-11-23)</p>
          </li>
        </ul>
        -->
              
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td>
              <heading>Education</heading>
            </td>
          </tr>
        </table>

        <table width="100%" align="center" border="0" cellpadding="10">
          <tr>
            <td width="100%" valign="top">
              <p>
                <strong>Aug. 2017 - Aug. 2021 </strong>, Information Engineering,
                <i>
                  <b>the Chinese University of Hong Kong</b>
                </i></p>
              <p>Doctor of Philosophy</br>
              </p>
            </td>
          </tr>
        </table>

        <table width="100%" align="center" border="0" cellpadding="10">
          <tr>
            <td width="100%" valign="top">
              <p>
                <strong>Sep. 2013 - Jul. 2017 </strong>, Automation,
                <i>
                  <b>Nanjing University</b>
                </i></p>
              <p>Bachelor of Engineering</br>
              </p>
            </td>
          </tr>
        </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td>
              <heading>Work Experience</heading>
            </td>
          </tr>
        </table>

       <table width="100%" align="center" border="0" cellpadding="10">
          <tr>
            <td width="100%" valign="top">
              <p>
                <strong>Jan. 2023 - now </strong>,
                <i>
                  work in <b>Shanghai AI Lab (OpenGVLab)</b>
                </i>
              </p>
            </td>
          </tr>
        </table>

        <table width="100%" align="center" border="0" cellpadding="10">
          <tr>
            <td width="100%" valign="top">
              <p>
                <strong>Aug. 2021 - Dec. 2022 </strong>,
                <i>
                  work in <b>Shanghai AI Lab (ADG)</b>
                </i>
              </p>
            </td>
          </tr>
        </table>

        <table width="100%" align="center" border="0" cellpadding="10">
          <tr>
            <td width="100%" valign="top">
              <p>
                <strong>Apr. 2020 - May. 2020 </strong>,
                <i>
                  internship in <b>SenseTime</b>
                </i>
              </p>
            </td>
          </tr>
        </table>

        <table width="100%" align="center" border="0" cellpadding="10">
          <tr>
            <td width="100%" valign="top">
              <p>
                <strong>Mar. 2017 - May. 2017 </strong>,
                <i>
                  internship in <b>SenseTime</b>
                </i>
              </p>
            </td>
          </tr>
        </table>


        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="100%" valign="middle">
              <heading>Publications</heading>
            </td>
          </tr>
        </table>

        <p>*: Corresponding Author(s)</p>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
          <tr>
            <td width="100%" valign="top">
              <p>
                <papertitle>Vision-Centric BEV Perception: A Survey</papertitle>
                <br>
                Yuexin Ma, Tai Wang, Xuyang Bai, Huitong Yang, <strong>Yuenan Hou</strong>, Yaming Wang, Yu Qiao, Ruigang Yang, Xinge Zhu
                <br>
                <em>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</em>, 2024
                <br>
                <a href="https://arxiv.org/abs/2208.02797">[pdf]</a>
                <a href="https://github.com/4DVLab/Vision-Centric-BEV-Perception">[code]</a>
                <img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/4DVLab/Vision-Centric-BEV-Perception?style=social">
                <img alt="GitHub forks" style="vertical-align:middle" src="https://img.shields.io/github/forks/4DVLab/Vision-Centric-BEV-Perception?style=social">
                <br>
            </td>
          </tr>
        </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
          <tr>
            <td width="100%" valign="top">
              <p>
                <papertitle>Predicting Gradient is Better: Exploring Self-Supervised Learning for SAR ATR with a Joint-Embedding Predictive Architecture</papertitle>
                <br>
                Weijie Li, Wei Yang, Tianpeng Liu, <strong>Yuenan Hou</strong>, Yuxuan Li, Zhen Liu, Yongxiang Liu, Li Liu
                <br>
                <em>ISPRS Journal of Photogrammetry and Remote Sensing</em>, 2024
                <br>
                <a href="https://www.sciencedirect.com/science/article/pii/S0924271624003514">[pdf]</a>
                <br>
            </td>
          </tr>
        </table>
  
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
          <tr>
            <td width="100%" valign="top">
              <p>
                <papertitle>TASeg: Temporal Aggregation Network for LiDAR Semantic Segmentation</papertitle>
                <br>
                Xiaopei Wu, <strong>Yuenan Hou*</strong>, Xiaoshui Huang*, Binbin Lin, Tong He, Xinge Zhu, Yuexin Ma, Boxi Wu, Haifeng Liu*, Deng Cai, Wanli Ouyang
                <br>
                <em>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2024
                <br>
                <a href="https://openaccess.thecvf.com/content/CVPR2024/html/Wu_TASeg_Temporal_Aggregation_Network_for_LiDAR_Semantic_Segmentation_CVPR_2024_paper.html">[pdf]</a>
                <br>
            </td>
          </tr>
        </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
          <tr>
            <td width="100%" valign="top">
              <p>
                <papertitle>WildRefer: 3D Object Localization in Large-scale Dynamic Scenes with Multi-modal Visual Data and Natural Language</papertitle>
                <br>
                Zhenxiang Lin, Xidong Peng, Peishan Cong, <strong>Yuenan Hou</strong>, Xinge Zhu, Sibei Yang, Yuexin Ma
                <br>
                <em>European Conference on Computer Vision</em>, 2024
                <br>
                <a href="https://arxiv.org/abs/2304.05645">[pdf]</a>
                <br>
            </td>
          </tr>
        </table>
  
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
          <tr>
            <td width="100%" valign="top">
              <p>
                <papertitle>Point Cloud Pre-training with Diffusion Models</papertitle>
                <br>
                Xiao Zheng, Xiaoshui Huang, Guofeng Mei, <strong>Yuenan Hou</strong>, Zhaoyang Lyu, Bo Dai, Wanli Ouyang, Yongshun Gong
                <br>
                <em>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2024
                <br>
                <a href="https://arxiv.org/abs/2311.14960">[pdf]</a>
                <br>
            </td>
          </tr>
        </table>
  
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
          <tr>
            <td width="100%" valign="top">
              <p>
                <papertitle>EPCL: Frozen CLIP Transformer is An Efficient Point Cloud Encoder</papertitle>
                <br>
                Xiaoshui Huang, Zhou Huang, Sheng Li, Wentao Qu, Tong He, <strong>Yuenan Hou</strong>, Yifan Zuo, Wanli Ouyang
                <br>
                <em>AAAI Conference on Artificial Intelligence (AAAI, <font color="ff0000">Oral</font>)</em>, 2024
                <br>
                <a href="https://arxiv.org/abs/2212.04098">[pdf]</a>
                <br>
            </td>
          </tr>
        </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
          <tr>
            <td width="100%" valign="top">
              <p>
                <papertitle>Semi-Supervised 3D Object Detection with PatchTeacher and PillarMix</papertitle>
                <br>
                Xiaopei Wu, Liang Peng, Liang Xie, <strong>Yuenan Hou</strong>, Binbin Lin, Xiaoshui Huang, Haifeng Liu, Deng Cai, Wanli Ouyang
                <br>
                <em>AAAI Conference on Artificial Intelligence (AAAI)</em>, 2024
                <br>
                <a href="https://ojs.aaai.org/index.php/AAAI/article/view/28432">[pdf]</a>
                <br>
            </td>
          </tr>
        </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
          <tr>
            <td width="100%" valign="top">
              <p>
                <papertitle>Advances in 3D Pre-Training and Downstream Tasks: A Survey</papertitle>
                <br>
                <strong>Yuenan Hou*</strong>, Xiaoshui Huang, Shixiang Tang, Tong He, Wanli Ouyang
                <br>
                <em>Vicinagearth</em>, 2024
                <br>
                <a href="https://link.springer.com/article/10.1007/s44336-024-00007-4">[pdf]</a>
                <br>
            </td>
          </tr>
        </table>
  
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
          <tr>
            <td width="100%" valign="top">
              <p>
                <papertitle>CluB: Cluster Meets BEV for LiDAR-Based 3D Object Detection</papertitle>
                <br>
                Yingjie Wang, Jiajun Deng, <strong>Yuenan Hou</strong>, Yao Li, Yu Zhang, Jianmin Ji, Wanli Ouyang, Yanyong Zhang
                <br>
                <em>Neural Information Processing Systems (NeurIPS)</em>, 2023
                <br>
                <a href="https://openreview.net/forum?id=jIhX7SpfCz">[pdf]</a>
                <br>
            </td>
          </tr>
        </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
          <tr>
            <td width="100%" valign="top">
              <p>
                <papertitle>RangePerception: Taming LiDAR Range View for Efficient and Accurate 3D Object Detection</papertitle>
                <br>
                Yeqi Bai, Ben Fei, Youquan Liu, Tao Ma, <strong>Yuenan Hou</strong>, Botian Shi, Yikang Li
                <br>
                <em>Neural Information Processing Systems (NeurIPS)</em>, 2023
                <br>
                <a href="https://openreview.net/forum?id=9kFQEJSyCM">[pdf]</a>
                <br>
            </td>
          </tr>
        </table>
  
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
          <tr>
            <td width="100%" valign="top">
              <p>
                <papertitle>Rethinking Range View Representation for LiDAR Segmentation</papertitle>
                <br>
                Lingdong Kong, Youquan Liu, Runnan Chen, Yuexin Ma, Xinge Zhu, Yikang Li, <strong>Yuenan Hou*</strong>, Yu Qiao, Ziwei Liu*
                <br>
                <em>International Conference on Computer Vision (ICCV)</em>, 2023
                <br>
                <a href="https://arxiv.org/abs/2303.05367">[pdf]</a>
                <br>
            </td>
          </tr>
        </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
          <tr>
            <td width="100%" valign="top">
              <p>
                <papertitle>UniSeg: A Unified Multi-Modal LiDAR Segmentation Network and the OpenPCSeg Codebase</papertitle>
                <br>
                Youquan Liu, Runnan Chen, Xin Li, Lingdong Kong, Yuchen Yang, Zhaoyang Xia, Yeqi Bai*, Yuexin Ma, Xinge Zhu, Yikang Li*, Yu Qiao, <strong>Yuenan Hou*</strong>
                <br>
                <em>International Conference on Computer Vision (ICCV)</em>, 2023
                <br>
                <a href="https://arxiv.org/abs/2309.05573">[pdf]</a>
                <a href="https://github.com/PJLab-ADG/PCSeg">[code]</a>
                <img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/PJLab-ADG/PCSeg?style=social">
                <img alt="GitHub forks" style="vertical-align:middle" src="https://img.shields.io/github/forks/PJLab-ADG/PCSeg?style=social">
                <br>
            </td>
          </tr>
        </table>
  
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
          <tr>
            <td width="100%" valign="top">
              <p>
                <papertitle>Human-centric Scene Understanding for 3D Large-scale Scenarios</papertitle>
                <br>
                Yiteng Xu, Peishan Cong, Yichen Yao, Runnan Chen, <strong>Yuenan Hou</strong>, Xinge Zhu, Xuming He, Jingyi Yu, Yuexin Ma
                <br>
                <em>International Conference on Computer Vision (ICCV)</em>, 2023
                <br>
                <a href="https://arxiv.org/abs/2307.14392">[pdf]</a>
                <br>
            </td>
          </tr>
        </table>
  
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
          <tr>
            <td width="100%" valign="top">
              <p>
                <papertitle>See More and Know More: Zero-shot Point Cloud Segmentation via Multi-modal Visual Data</papertitle>
                <br>
                Yuhang Lu, Qi Jiang, Runnan Chen, <strong>Yuenan Hou</strong>, Xinge Zhu, Yuexin Ma
                <br>
                <em>International Conference on Computer Vision (ICCV)</em>, 2023
                <br>
                <a href="https://arxiv.org/abs/2307.10782">[pdf]</a>
                <br>
            </td>
          </tr>
        </table>

  
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
          <tr>
            <td width="100%" valign="top">
              <p>
                <papertitle>SCPNet: Semantic Scene Completion on Point Cloud</papertitle>
                <br>
                Zhaoyang Xia, Youquan Liu, Xin Li, Xinge Zhu, Yuexin Ma, Yikang Li, <strong>Yuenan Hou*</strong>, Yu Qiao
                <br>
                <em>IEEE Conference on Computer Vision and Pattern Recognition (CVPR, <font color="ff0000">Highlight</font>, top 10%)</em>, 2023
                <br>
                <a href="https://arxiv.org/abs/2303.06884">[pdf]</a>
                <a href="https://github.com/SCPNet/Codes-for-SCPNet">[code]</a>
                <img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/SCPNet/Codes-for-SCPNet?style=social">
                <br>
            </td>
          </tr>
        </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
          <tr>
            <td width="100%" valign="top">
              <p>
                <papertitle>CLIP2Scene: Towards Label-efficient 3D Scene Understanding by CLIP</papertitle>
                <br>
                Runnan Chen, Youquan Liu, Lingdong Kong, Xinge Zhu, Yuexin Ma, Yikang Li, <strong>Yuenan Hou*</strong>, Yu Qiao, Wenping Wang*
                <br>
                <em>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2023
                <br>
                <a href="https://arxiv.org/abs/2301.04926">[pdf]</a>
                <a href="https://github.com/runnanchen/CLIP2Scene">[code]</a>
                <img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/runnanchen/CLIP2Scene?style=social">                
                <br>
            </td>
          </tr>
        </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
          <tr>
            <td width="100%" valign="top">
              <p>
                <papertitle>LoGoNet: Towards Accurate 3D Object Detection with Local-to-Global Cross-Modal Fusion</papertitle>
                <br>
                Xin Li, Tao Ma, <strong>Yuenan Hou</strong>, Botian Shi, Yuchen Yang, Youquan Liu, Xingjiao Wu, Qin Chen, Yikang Li, Yu Qiao, Liang He
                <br>
                <em>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2023
                <br>
                <a href="https://arxiv.org/abs/2303.03595v2">[pdf]</a>
                <a href="https://github.com/PJLab-ADG/LoGoNet">[code]</a>
                <img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/PJLab-ADG/LoGoNet?style=social"> 
                <img alt="GitHub forks" style="vertical-align:middle" src="https://img.shields.io/github/forks/PJLab-ADG/LoGoNet?style=social">
                <br>
                <p>
                  Rank <font color="ff0000">1st</font> in Waymo 3D Object Detection Challenge (1/100)!!!
                </p>
            </td>
          </tr>

        </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
          <tr>
            <td width="100%" valign="top">
              <p>
                <papertitle>Network Pruning via Resource Reallocation</papertitle>
                <br>
                <strong>Yuenan Hou</strong>, Zheng Ma, Chunxiao Liu, Zhe Wang, Chen Change Loy
                <br>
                <em>Pattern Recognition</em>, 2023
                <br>
                <a href="https://arxiv.org/abs/2103.01847">[pdf]</a>
                <a href="https://github.com/cardwing/Codes-for-PEEL">[code]</a>
                <!–– <img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/cardwing/Codes-for-Lane-Detection?style=social">
                <!–– <img alt="GitHub forks" style="vertical-align:middle" src="https://img.shields.io/github/forks/cardwing/Codes-for-Lane-Detection?style=social">
                <br>
            </td>
          </tr>
        </table>
         
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
          <tr>
            <td width="100%" valign="top">
              <p>
                <papertitle>Gradient Modulated Contrastive Distillation of Low-Rank Multi-Modal Knowledge for Disease Diagnosis</papertitle>
                <br>
                Xiaohan Xing, Zhen Chen, <strong>Yuenan Hou</strong>, Yixuan Yuan
                <br>
                <em>Medical Image Analysis (MedIA)</em>, 2023
                <br>
                <a href="https://www.sciencedirect.com/science/article/abs/pii/S1361841523001342">[pdf]</a>
                <br>
            </td>
          </tr>
        </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
          <tr>
            <td width="100%" valign="top">
              <p>
                <papertitle>How to Tame Mobility in Federated Learning over Mobile Networks?</papertitle>
                <br>
                Yan Peng, Xiaogang Tang, Yiqing Zhou, <strong>Yuenan Hou</strong>, Jintao Li, Yanli Qi, Ling Liu, Hai Lin
                <br>
                <em>IEEE Transactions on Wireless Communications (TWC)</em>, 2023
                <br>
                <a href="https://ieeexplore.ieee.org/document/10128968">[pdf]</a>
                <br>
            </td>
          </tr>
        </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
          <tr>
            <td width="100%" valign="top">
              <p>
                <papertitle>Point-to-Voxel Knowledge Distillation for LiDAR Semantic Segmentation</papertitle>
                <br>
                <strong>Yuenan Hou</strong>, Xinge Zhu, Yuexin Ma, Chen Change Loy, Yikang Li
                <br>
                <em>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2022
                <br>
                <a href="https://arxiv.org/abs/2206.02099">[pdf]</a>
                <a href="https://github.com/cardwing/Codes-for-PVKD">[code]</a>
                <img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/cardwing/Codes-for-PVKD?style=social"> 
                <img alt="GitHub forks" style="vertical-align:middle" src="https://img.shields.io/github/forks/cardwing/Codes-for-PVKD?style=social">
                <br>
                <p>
                  Rank <font color="ff0000">1st</font> in Waymo 2022 3D Semantic Segmentation Challenge and SemanticKITTI LiDAR Semantic Segmentation Challenge (single-scan)!!!
                </p>
            </td>
          </tr>
        </table>


     <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
          <tr>
            <td width="100%" valign="top">
              <p>
                <papertitle>Homogeneous Multi-modal Feature Fusion and Interaction for 3D Object Detection</papertitle>
                <br>
                Xin Li, Botian Shi, <strong>Yuenan Hou</strong>, Xingjiao Wu, Tianlong Ma, Yikang Li, Liang He
                <br>
                <em>European Conference on Computer Vision</em>, 2022
                <br>
                <a href="https://arxiv.org/abs/2210.09615">[pdf]</a>
                <a href="https://github.com/sankin97/HMFI">[code]</a>
                <img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/sankin97/HMFI?style=social">
                <br>
            </td>
          </tr> 
       </table>

     <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
          <tr>
            <td width="100%" valign="top">
              <p>
                <papertitle>Mind the Gap in Distilling StyleGANs</papertitle>
                <br>
                Guodong Xu, <strong>Yuenan Hou</strong>, Ziwei Liu, Chen Change Loy
                <br>
                <em>European Conference on Computer Vision</em>, 2022
                <br>
                <a href="https://arxiv.org/abs/2208.08840">[pdf]</a>
                <a href="https://github.com/xuguodong03/StyleKD">[code]</a>
                <img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/xuguodong03/StyleKD?style=social">
                <br>
            </td>
          </tr> 
       </table>
       
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
          <tr>
            <td width="100%" valign="top">
              <p>
                <papertitle>STCrowd: A Multimodal Dataset for Pedestrian Perception in Crowded Scenes</papertitle>
                <br>
                Peishan Cong, Xinge Zhu, Feng Qiao, Yiming Ren, Xidong Peng, <strong>Yuenan Hou</strong>, Lan Xu, Ruigang Yang, Dinesh Manocha, Yuexin Ma
                <br>
                <em>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2022
                <br>
                <a href="https://arxiv.org/abs/2204.01026">[pdf]</a>
                <a href="https://github.com/4DVLab/STCrowd">[code]</a>
                <img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/4DVLab/STCrowd?style=social">
                <br>
            </td>
          </tr>
        </table>


        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
          <tr>
            <td width="100%" valign="top">
              <p>
                <papertitle>Discrepancy and Gradient-guided Multi-modal Knowledge Distillation for Pathological Glioma Grading</papertitle>
                <br>
                Xiaohan Xing, Zhen Chen, Meilu Zhu, <strong>Yuenan Hou</strong>, Zhifan Gao, Yixuan Yuan
                <br>
                <em>International Conference on Medical Image Computing and Computer Assisted Intervension (MICCAI)</em>, <a href="https://miccai.org/index.php/about-miccai/awards/best-paper-award-and-young-scientist-award/"><font color="ff0000">MICCAI Young Scientist Award</font></a>, 2022
                <br>
                <a href="https://link.springer.com/chapter/10.1007/978-3-031-16443-9_61">[pdf]</a>
                <a href="https://github.com/CityU-AIM-Group/MultiModal-learning">[code]</a>
                <img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/CityU-AIM-Group/MultiModal-learning?style=social">
                <br>
            </td>
          </tr>
        </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
          <tr>
            <td width="100%" valign="top">
              <p>
                <papertitle>Categorical Relation-Preserving Contrastive Knowledge Distillation for Medical Image Classification</papertitle>
                <br>
                Xiaohan Xing, <strong>Yuenan Hou</strong>, Hang Li, Yixuan Yuan, Hongsheng Li, Max Q.-H. Meng
                <br>
                <em>International Conference on Medical Image Computing and Computer Assisted Intervension (MICCAI)</em>, 2021
                <br>
                <a href="https://arxiv.org/abs/2107.03225v1">[pdf]</a>
                <a href="https://github.com/hathawayxxh/CRCKD">[code]</a>
                <img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/hathawayxxh/CRCKD?style=social">
                <br>
            </td>
          </tr>
        </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
          <tr>
            <td width="100%" valign="top">
              <p>
                <papertitle>Inter-Region Affinity Distillation for Road Marking Segmentation</papertitle>
                <br>
                <strong>Yuenan Hou</strong>, Zheng Ma, Chunxiao Liu, Tak-Wai Hui, Chen Change Loy
                <br>
                <em>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2020
                <br>
                <a href="https://arxiv.org/abs/2004.05304">[pdf]</a>
                <a href="https://github.com/cardwing/Codes-for-IntRA-KD">[code]</a>
                <img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/cardwing/Codes-for-IntRA-KD?style=social">
                <img alt="GitHub forks" style="vertical-align:middle" src="https://img.shields.io/github/forks/cardwing/Codes-for-IntRA-KD?style=social">
                <br>
            </td>
          </tr>
        </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
          <tr>
            <td width="100%" valign="top">
              <p>
                <papertitle>Learning Lightweight Lane Detection CNNs by Self Attention Distillation</papertitle>
                <br>
                <strong>Yuenan Hou</strong>, Zheng Ma, Chunxiao Liu, Chen Change Loy
                <br>
                <em>International Conference on Computer Vision (ICCV)</em>, 2019
                <br>
                <a href="https://arxiv.org/abs/1908.00821">[pdf]</a>
                <a href="https://github.com/cardwing/Codes-for-Lane-Detection">[code]</a>
                <img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/cardwing/Codes-for-Lane-Detection?style=social">
                <img alt="GitHub forks" style="vertical-align:middle" src="https://img.shields.io/github/forks/cardwing/Codes-for-Lane-Detection?style=social">
                <br>
                <p>
                  One of the <font color="ff0000">Most Influential</font> paper in the lane detection field!
                </p>
            </td>
          </tr>
        </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
          <tr>
            <td width="100%" valign="top">
              <p>
                <papertitle>Learning to Steer by Mimicking Features from Heterogeneous Auxiliary Networks</papertitle>
                <br>
                <strong>Yuenan Hou</strong>, Zheng Ma, Chunxiao Liu, Chen Change Loy
                <br>
                <em>AAAI Conference on Artificial Intelligence (AAAI, <font color="ff0000">Oral</font>)</em>, 2019
                <br>
                <a href="https://arxiv.org/abs/1811.02759">[pdf]</a>
                <a href="https://cardwing.github.io/projects/FM-Net">[project page]</a>
                <a href="https://github.com/cardwing/Codes-for-Steering-Control">[code]</a>
                <img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/cardwing/Codes-for-Steering-Control?style=social">
                <img alt="GitHub forks" style="vertical-align:middle" src="https://img.shields.io/github/forks/cardwing/Codes-for-Steering-Control?style=social">
                <br>
            </td>
          </tr>
        </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
          <tr>
            <td width="100%" valign="top">
              <p>
                <papertitle>A Novel DDPG Method with Prioritized Experience Replay</papertitle>
                <br>
                <strong>Yuenan Hou</strong>, Lifeng Liu, Qing Wei, Xudong Xu, Chunlin Chen
                <br>
                <em>IEEE International Conference on Systems, Man, and Cybernetics (SMC)</em>, <font color="ff0000">Most Cited Papers</font> (3/650), 2017
                <br>
                <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8122622&tag=1">[pdf]</a>
                <a href="https://github.com/cardwing/Codes-for-RL-PER">[code]</a>
                <img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/cardwing/Codes-for-RL-PER?style=social">
                <img alt="GitHub forks" style="vertical-align:middle" src="https://img.shields.io/github/forks/cardwing/Codes-for-RL-PER?style=social">
                <br>
            </td>
          </tr>
        </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="100%" valign="middle">
              <heading>Preprint</heading>
            </td>
          </tr>
        </table>

     <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">

          <tr>
            <td width="100%" valign="top">
              <p>
                <papertitle>OccMamba: Semantic Occupancy Prediction with State Space Models</papertitle>
                <br>
                Heng Li, <strong>Yuenan Hou*</strong>, Xiaohan Xing, Xiao Sun, Yanyong Zhang*
                <br>
                <em>arXiv preprint arXiv:2408.09859</em>, 2024
                <br>
                <a href="https://arxiv.org/abs/2408.09859">[pdf]</a>
                <br>
            </td>
          </tr>       
       
          <tr>
            <td width="100%" valign="top">
              <p>
                <papertitle>SARATR-X: A Foundation Model for Synthetic Aperture Radar Images Target Recognition</papertitle>
                <br>
                Weijie Li, Wei Yang, <strong>Yuenan Hou</strong>, Li Liu, Yongxiang Liu, Xiang Li
                <br>
                <em>arXiv preprint arXiv:2405.09365</em>, 2024
                <br>
                <a href="https://arxiv.org/abs/2405.09365">[pdf]</a>
                <br>
            </td>
          </tr>
       
          <tr>
            <td width="100%" valign="top">
              <p>
                <papertitle>NeRF-Det++: Incorporating Semantic Cues and Perspective-aware Depth Supervision for Indoor Multi-View 3D Detection</papertitle>
                <br>
                Chenxi Huang, <strong>Yuenan Hou*</strong>, Weicai Ye, Di Huang, Xiaoshui Huang, Binbin Lin*, Deng Cai, Wanli Ouyang
                <br>
                <em>arXiv preprint arXiv:2402.14464</em>, 2024
                <br>
                <a href="https://arxiv.org/abs/2402.14464">[pdf]</a>
                <a href="https://github.com/mrsempress/NeRF-Detplusplus">[code]</a>
                <br>
            </td>
          </tr>
       
          <tr>
            <td width="100%" valign="top">
              <p>
                <papertitle>A Comprehensive Survey on 3D Content Generation</papertitle>
                <br>
                Jian Liu, Xiaoshui Huang, Tianyu Huang, Lu Chen, <strong>Yuenan Hou</strong>, Shixiang Tang, Ziwei Liu, Wanli Ouyang, Wangmeng Zuo, Junjun Jiang, Xianming Liu
                <br>
                <em>arXiv preprint arXiv:2402.01166</em>, 2024
                <br>
                <a href="https://arxiv.org/abs/2402.01166">[pdf]</a>
                <a href="https://github.com/hitcslj/Awesome-AIGC-3D">[code]</a>
                <img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/hitcslj/Awesome-AIGC-3D?style=social">
                <img alt="GitHub forks" style="vertical-align:middle" src="https://img.shields.io/github/forks/hitcslj/Awesome-AIGC-3D?style=social">
                <br>
            </td>
          </tr>

          <tr>
            <td width="100%" valign="top">
              <p>
                <papertitle>Uni3D-LLM: Unifying Point Cloud Perception, Generation and Editing with Large Language Models</papertitle>
                <br>
                Dingning Liu, Xiaoshui Huang, <strong>Yuenan Hou</strong>, Zhihui Wang, Zhenfei Yin, Yongshun Gong, Peng Gao, Wanli Ouyang
                <br>
                <em>arXiv preprint arXiv:2402.03327</em>, 2024
                <br>
                <a href="https://arxiv.org/abs/2402.03327">[pdf]</a>
                <br>
            </td>
          </tr>

          <tr>
            <td width="100%" valign="top">
              <p>
                <papertitle>Self-Supervised Learning for SAR ATR with a Knowledge-Guided Predictive Architecture</papertitle>
                <br>
                Weijie Li, Yang Wei, Tianpeng Liu, <strong>Yuenan Hou</strong>, Yongxiang Liu, Li Liu
                <br>
                <em>arXiv preprint arXiv:2311.15153</em>, 2023
                <br>
                <a href="https://arxiv.org/abs/2311.15153">[pdf]</a>
                <br>
            </td>
          </tr>
       
          <tr>
            <td width="100%" valign="top">
              <p>
                <papertitle>Clothes-Invariant Feature Learning by Causal Intervention for Clothes-Changing Person Re-identification</papertitle>
                <br>
                Xulin Li, Yan Lu, Bin Liu, <strong>Yuenan Hou</strong>, Yating Liu, Qi Chu, Wanli Ouyang, Nenghai Yu
                <br>
                <em>arXiv preprint arXiv:2305.06145</em>, 2023
                <br>
                <a href="https://arxiv.org/abs/2305.06145">[pdf]</a>
                <br>
            </td>
          </tr>
       
          <tr>
            <td width="100%" valign="top">
              <p>
                <papertitle>OpenPCSeg: An Open Source Point Cloud Segmentation Codebase</papertitle>
                <br>
                Youquan Liu, Yeqi Bai, Lingdong Kong, Runnan Chen, <strong>Yuenan Hou</strong>, Botian Shi, Yikang Li
                <br>
                Good performance on SemanticKITTI, ScribbleKITTI and Waymo.
                <br>
                <a href="https://github.com/PJLab-ADG/OpenPCSeg">[code]</a>
                <img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/PJLab-ADG/OpenPCSeg?style=social">
                <img alt="GitHub forks" style="vertical-align:middle" src="https://img.shields.io/github/forks/PJLab-ADG/OpenPCSeg?style=social">
                <br>
            </td>
          </tr>                            
       
          <tr>
            <td width="100%" valign="top">
              <p>
                <papertitle>ICPV: Deep Fusion of Different Point Cloud Representations</papertitle>
                <br>
                Hao Tian, <strong>Yuenan Hou</strong>, Huijie Wang, Youquan Liu, Jiawei Li, Xinge Zhu, Wenkang Qin, Junchao Gong, Yang Li, Kai Li
                <br>
                <em><font color="ff0000">1st</font> Place Solution for 3D Semantic Segmentation Track in Waymo Open Dataset Challenge</em>, 2022
                <br>
                <a href="https://storage.googleapis.com/waymo-uploads/files/research/3DSemSeg/3DSemseg_Cylinder3D.pdf">[pdf]</a>
                <a href="https://github.com/cardwing/Codes-for-PVKD">[code]</a>
                <img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/cardwing/Codes-for-PVKD?style=social">
                <img alt="GitHub forks" style="vertical-align:middle" src="https://img.shields.io/github/forks/cardwing/Codes-for-PVKD?style=social">
                <br>
            </td>
          </tr>

          <tr>
            <td width="100%" valign="top">
              <p>
                <papertitle>Agnostic Lane Detection</papertitle>
                <br>
                <strong>Yuenan Hou</strong>
                <br>
                <em>arXiv preprint arXiv:1905.03704</em>, 2019
                <br>
                <a href="https://www.researchgate.net/publication/332528412_Agnostic_Lane_Detection">[pdf]</a>
                <a href="https://github.com/cardwing/Codes-for-Lane-Detection">[code]</a>
                <img alt="GitHub stars" style="vertical-align:middle" src="https://img.shields.io/github/stars/cardwing/Codes-for-Lane-Detection?style=social">
                <img alt="GitHub forks" style="vertical-align:middle" src="https://img.shields.io/github/forks/cardwing/Codes-for-Lane-Detection?style=social">
                <br>
            </td>
          </tr>

        </table>

        <!--
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="100%" valign="middle">
              <heading>Selected Course Work</heading>
            </td>
          </tr>
        </table>
        <ul>
          <li>
            <p><a href="https://cardwing.github.io/files/RL_course_report.pdf">Improving DDPG via Prioritized Experience Replay</a>, <a href="https://course.ie.cuhk.edu.hk/~ierg6130/index.html">IERG 6130 Reinforcement Learning</a></p>
          </li>
        </ul>
        -->  

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="100%" valign="middle">
              <heading>Current Students</heading>
            </td>
          </tr>
        </table>
        <ul>
          <li>
            <p><a href="https://scholar.google.com/citations?hl=en&user=Z2rzIoAAAAAJ">Xiaopei Wu</a> (PhD at ZJU)</p>
          </li>
        </ul>
        <ul>
          <li>
            <p><a href="">Haoxuan Ma</a> (SEU)</p>
          </li>
        </ul>
        <ul>
          <li>
            <p><a href="">Sizhe Zheng</a> (NUAA)</p>
          </li>
        </ul>
        <ul>
          <li>
            <p><a href="">Zhiting Zhou</a> (THU)</p>
          </li>
        </ul>
        <ul>
          <li>
            <p><a href="">Yulin Chen</a> (SEU)</p>
          </li>
        </ul>
        <ul>
          <li>
            <p><a href="">Heng Li</a> (PhD at USTC)</p>
          </li>
        </ul>
        <ul>
          <li>
            <p><a href="">Weijie Li</a> (PhD at NUDT)</p>
          </li>
        </ul>
        <ul>
          <li>
            <p><a href="">Jiaojiao Su</a> (CSU)</p>
          </li>
        </ul>
        <ul>
          <li>
            <p><a href="">Zijia Chen</a> (Master at NUDT)</p>
          </li>
        </ul>
        <ul>
          <li>
            <p><a href="">Xia Chu</a> (SJTU)</p>
          </li>
        </ul>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="100%" valign="middle">
              <heading>Previous Mentorship</heading>
            </td>
          </tr>
        </table>
        <ul>
          <li>
            <p><a href="">Nange Wang</a> (SJTU)</p>
          </li>
        </ul>
        <ul>
          <li>
            <p><a href="https://scholar.google.com/citations?hl=en&user=e14HvOcAAAAJ">Chenxi Huang</a> (PhD at ZJU)</p>
          </li>
        </ul>
        <ul>
          <li>
            <p><a href="https://scholar.google.com/citations?hl=en&user=fdDOA-0AAAAJ">Zeren Chen</a> (PhD at BUAA)</p>
          </li>
        </ul>
        <ul>
          <li>
            <p><a href="https://scholar.google.com/citations?user=r7t3068AAAAJ">Yingjie Wang</a> (PhD at USTC)</p>
          </li>
        </ul>
        <ul>
          <li>
            <p><a href="">Lidian Wang</a> (PhD at USTC)</p>
          </li>
        </ul>
        <ul>
          <li>
            <p><a href="">Jingyuan Xie</a> (THU)</p>
          </li>
        </ul>
        <ul>
          <li>
            <p><a href="">Jiaao Tang</a> (THU)</p>
          </li>
        </ul>
        <ul>
          <li>
            <p><a href="https://sankin97.github.io/">Xin Li</a> (PostDoc at SJTU)</p>
          </li>
        </ul>
        <ul>
          <li>
            <p><a href="https://scholar.google.com/citations?hl=en&user=cZXBV2cAAAAJ">Zhaoyang Xia</a> (Researcher at SH AI Lab)</p>
          </li>
        </ul>
        <ul>
          <li>
            <p><a href="https://scholar.google.com/citations?hl=en&user=_fl950wAAAAJ">Yeqi Bai</a> (NTU)</p>
          </li>
        </ul>
        <ul>
          <li>
            <p><a href="https://scholar.google.com/citations?user=Uq2DuzkAAAAJ&hl=en">Runnan Chen</a> (PostDoc at USYD)</p>
          </li>
        </ul>
        <ul>
          <li>
            <p><a href="https://ldkong.com/">Lingdong Kong</a> (PhD at NUS)</p>
          </li>
        </ul>
        <ul>
          <li>
            <p><a href="">Kai Li</a> (Master at SUSTC)</p>
          </li>
        </ul>
        <ul>
          <li>
            <p><a href="https://scholar.google.com/citations?hl=en&user=ifu71fEAAAAJ">Junchao Gong</a> (PhD at SJTU)</p>
          </li>
        </ul>
        <ul>
          <li>
            <p><a href="https://scholar.google.com/citations?user=J9a48hMAAAAJ&hl=en">Youquan Liu</a> (PhD at Fudan)</p>
          </li>
        </ul>  

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td>
              <heading>Academic Services</heading>
            </td>
          </tr>
        </table>
        <ul>
          <li>
            <p>Area chair of PRCV 2023</p>
          </li>
        </ul>
        <ul>
          <li>
            <p>Conference reviewer of CVPR, ICCV, ECCV, ICLR, NeurIPS, AAAI, IJCAI, WACV</p>
          </li>
        </ul>
        <ul>
          <li>
            <p>Journal reviewer of TPAMI, IJCV, TIP, Neurocomputing, T-IV, RA-L, TCSVT, IET, Advanced Robotics, TGRS, JSTARS</p>
          </li>
        </ul>
        <ul>
          <li>
            <p>Invited talk at 2024 China Automatic Driving Conference, "Recent Advances, Opportunities and Challenges of Embodied AI", 2024</p>
          </li>
        </ul>
        <ul>
          <li>
            <p>Invited talk at Baowu Innovation Forum, China Baowu Steel Group, "Recent Advances, Opportunities and Challenges of Embodied AI", 2024</p>
          </li>
        </ul>
        <ul>
          <li>
            <p>Invited talk at <a href="https://sme.nju.edu.cn/1d/4c/c47266a728396/pagem.htm">Intelligent System and Control Forum</a>, School of Management & Engineering, Nanjing University, "Recent Advances, Opportunities and Challenges of Embodied AI", 2024</p>
          </li>
        </ul>
        <ul>
          <li>
            <p>Invited talk at Xiangjiang Forum, National University of Defense Technology, "Recent Advances, Opportunities and Challenges of Embodied AI", 2024</p>
          </li>
        </ul>
        <ul>
          <li>
            <p>Invited talk at Zhidongxi Open Classes, "Advances and Challenges of 3D Foundation Models in the Autonomous Driving Field", 2023</p>
          </li>
        </ul>
        <ul>
          <li>
            <p>Invited talk at School of Computer Science and Technology, East China Normal University, "Advances and Challenges of 3D Foundation Models in the Autonomous Driving Field", 2023</p>
          </li>
        </ul>
        <ul>
          <li>
            <p>Invited talk at School of Information Science and Technology, ShanghaiTech University, "Recent Advances and Challenges of 3D Perception Tasks", 2022</p>
          </li>
        </ul>
        <ul>
          <li>
            <p>Invited talk at School of Computer Science, Wuhan University, "Improving Deep Network Performance via Model Compression", 2021 <a href="https://cardwing.github.io/files/Improving_Deep_Network_Performance_via_Model_Compression.pdf">[ppt pdf]</a></p>
          </li>
        </ul>
        <ul>
          <li>
            <p>Invited talk at School of Cyber Science and Engineering, Wuhan University, "Improving Deep Network Performance via Model Compression", 2021</p>
          </li>
        </ul>
        <ul>
          <li>
            <p>Invited talk at School of Artificial Intelligence, Sun Yat-sen University, "Improving Deep Network Performance via Model Compression", 2020</p>
          </li>
        </ul>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td>
              <heading>Honors and Awards</heading>
            </td>
          </tr>
        </table>
        <ul>
          <li>
            <p>Most cited papers in <a href="https://ieeexplore.ieee.org/xpl/conhome/8114675/proceeding?isnumber=8122565&sortType=paper-citations">IEEE SMC 2017</a> (CAA-A), 3/650, Dec. 2024</p>
          </li>
          <li>
            <p>Rank 1st in SemanticKITTI LiDAR Semantic Segmentation Challenge (multi-scan), 1/50, Apr. 2023</p>
          </li>
          <li>
            <p>Visiting Scholar, Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences, Mar. 2023</p>
          </li>
          <li>
            <p><a href="https://miccai.org/index.php/about-miccai/awards/best-paper-award-and-young-scientist-award/">MICCAI Young Scientist Award</a> (4th author), Dec. 2022</p>
          </li>
          <li>
            <p>Rank 1st in nuScenes LiDAR Semantic Segmentation Challenge, 1/25, the "UniSeg" entry, Oct. 2022</p>
          </li>
          <li>
            <p>Rank 1st in Waymo 3D Object Detection Challenge, 1/100, the "LoGoNet_Ens" entry, Oct. 2022</p>
          </li>          
          <li>
            <p>Rank 1st in SemanticKITTI Semantic Scene Completion Challenge, 1/19, the "SCPNet" entry, Oct. 2022</p>
          </li>    
          <li>
            <p>Rank 1st in SemanticKITTI Panoptic Segmentation Challenge, 1/30, the "UniSeg" entry, Jul. 2022</p>
          </li> 
          <li>
            <p>Rank 1st in SemanticKITTI 4D Panoptic Segmentation Challenge, 1/7, the "UniSeg" entry, Jul. 2022</p>
          </li> 
          <li>
            <p>Rank 1st in SemanticKITTI LiDAR Semantic Segmentation Challenge (single-scan), 1/300, the "Point-Voxel-KD" entry, Jun. 2022</p>
          </li>
          <li>
            <p>Rank 1st in Waymo 3D Semantic Segmentation Challenge, 1/20, the "Cylinder3D" entry, May 2022</p>
          </li> 
          <li>
            <p>Shanghai Leading Talent Program, Shanghai Government, Dec. 2021</p>
          </li>
          <li>
            <p>Shanghai Specially-Invited Expert, Shanghai Government, Dec. 2021</p>
          </li>  
          <li>
            <p>Rank 2nd in SemanticKITTI LiDAR Semantic Segmentation Challenge (multi-scan), 2/100, the "PVKD" entry, Dec. 2021</p>
          </li>    
          <li>
            <p>Rank 1st in ApolloScape Lane Segmentation Challenge, Baidu, 1/104, the "Codes-for-IntRA-KD" entry, Sep. 2020</p>
          </li>       
          <li>
            <p>Postgraduate Scholarship, the Chinese University of Hong Kong, 2017 ~ 2021</p>
          </li>
          <li>
            <p>Third Prize of Jiangsu Province for Undergraduate Thesis, Nanjing University, 2017 <a href="https://cardwing.github.io/files/131270027-侯跃南-陈春林.pdf">[paper pdf]</a> <a href="http://jyt.jiangsu.gov.cn/module/download/downfile.jsp?classid=0&filename=0b7f135a90de478b92b389b3b54c7312.pdf">[awards list]</a></p>
          </li>
          <li>
            <p>Zhenggang Scholarship, Nanjing University, 2017</p>
          </li>
          <li>
            <p>Mathematical Contest in Modeling (MCM), Meritorious Winner (13%), 2016 <a href="https://cardwing.github.io/files/49652.pdf">[paper pdf]</a></p>
          </li>   
          <li>
            <p>Liao's Scholarship, Nanjing University, 2015</p>
          </li>
          <li>
            <p>National Scholarship, Nanjing University, 2014</p>
          </li>
        </ul>
        <br />
        <!-- Footer ================================================== -->
        <hr>

        <footer class="footer">
          <div class="container">
            <p>
              &nbsp;&nbsp;&nbsp;Updated Dec. 2024</p>
            </p>
            <p align=right>
              <a href="https://ai.stanford.edu/~kaidicao/"> Page Template </a>
              <a class="pull-right" href="#">
                <center><a href="https://clustrmaps.com/site/1af29"  title="Visit tracker"><img src="//www.clustrmaps.com/map_v2.png?d=N4fuA3R36CqTde01_juk6Hi7ossGUo6W7m_PXVkPHGg&cl=ffffff" /></a></center>
              </a>
            </p>
          </div>
        </footer>

        <script type="text/javascript">
          var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
          document.write(unescape("%3Cscript src='" + gaJsHost +
            "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
        </script>
        <script type="text/javascript">
          try {
            var pageTracker = _gat._getTracker("UA-7580334-1");
            pageTracker._trackPageview();
          } catch (err) {}
        </script>
      </td>
    </tr>
  </table>
</body>

</html>
